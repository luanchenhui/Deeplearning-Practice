{"cells":[{"outputs":[{"output_type":"stream","text":"traffic_sign8499\r\n","name":"stdout"}],"execution_count":1,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"2580B6604270434C8FD7A24CBE73547D","scrolled":false}},{"outputs":[],"execution_count":2,"source":"# 查看个人持久化工作区文件\n# !ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"4249199EE55C459D8B9CB4D7B45ECF23","scrolled":false}},{"outputs":[],"execution_count":3,"source":"# 查看当前kernerl下的package\n# !pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"E5CDA58B36F047AD819CE8BCD1D2E94C","scrolled":false}},{"outputs":[],"execution_count":4,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"E4027B7F9384447CBF71CF378D005B85","scrolled":false}},{"metadata":{"id":"9C6A4A67B67B4399B548157040A73770","mdEditEnable":false},"cell_type":"markdown","source":"# **一、分析数据集特点**"},{"metadata":{"id":"5B82340576894F5389A309BD3DE74A73","mdEditEnable":false},"cell_type":"markdown","source":"首先观察数据，看看要识别的交通标志种类多少（62），以及每类图像有多少。"},{"metadata":{"id":"77E6CECC72DC496B89B67D96FBCA110B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"00000  00006  00012  00018  00024  00030  00036  00042\t00048  00054  00060\n00001  00007  00013  00019  00025  00031  00037  00043\t00049  00055  00061\n00002  00008  00014  00020  00026  00032  00038  00044\t00050  00056\n00003  00009  00015  00021  00027  00033  00039  00045\t00051  00057\n00004  00010  00016  00022  00028  00034  00040  00046\t00052  00058\n00005  00011  00017  00023  00029  00035  00041  00047\t00053  00059\ntime: 400 ms\n","name":"stdout"}],"source":"!ls /home/kesci/input/traffic_sign8499/traffic-sign/train","execution_count":5},{"metadata":{"id":"2B11492414C84C748CF6E2AC1DA180F2","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"00000  00006  00012  00018  00024  00030  00036  00042\t00048  00054  00060\n00001  00007  00013  00019  00025  00031  00037  00043\t00049  00055  00061\n00002  00008  00014  00020  00026  00032  00038  00044\t00050  00056\n00003  00009  00015  00021  00027  00033  00039  00045\t00051  00057\n00004  00010  00016  00022  00028  00034  00040  00046\t00052  00058\n00005  00011  00017  00023  00029  00035  00041  00047\t00053  00059\ntime: 398 ms\n","name":"stdout"}],"source":"!ls /home/kesci/input/traffic_sign8499/traffic-sign/test","execution_count":6},{"metadata":{"id":"04706F68308E4AD68FECF5DF18D513A1","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(62, 62)"},"execution_count":7},{"output_type":"stream","text":"time: 9.78 ms\n","name":"stdout"}],"source":"from glob import glob\ntrain_folds = glob('/home/kesci/input/traffic_sign8499/traffic-sign/train/*')\ntest_folds = glob('/home/kesci/input/traffic_sign8499/traffic-sign/test/*')\nlen(train_folds), len(test_folds)","execution_count":7},{"metadata":{"id":"DFD568E3946247F19647ADA9911A06A9","mdEditEnable":false},"cell_type":"markdown","source":"查看每个类别中图像的数量，以了解类别图像的均衡性。"},{"metadata":{"id":"2A29C2861F494C6A8338DA18E76EBC0F","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 26.3 ms\n","name":"stdout"}],"source":"len_train_folds = []\nlen_test_folds = []\nfor fold in train_folds:\n    fold_files = glob(fold+'/*.png')\n    len_train_folds.append(len(fold_files))\nfor fold in test_folds:\n    fold_files = glob(fold+'/*.png')\n    len_test_folds.append(len(fold_files))\n# len(len_train_folds), len(len_test_folds)\n","execution_count":8},{"metadata":{"id":"15F7A1623DA94BD88A00D0FF7009B5A7","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 1200x600 with 2 Axes>"}},{"output_type":"stream","text":"time: 827 ms\n","name":"stdout"}],"source":"# train_folds[0].split('/')[7]\n# !pip install pyecharts\n''' 不堆叠排列 '''\n# import pyecharts\nlabels = [int(train_folds[idx].split('/')[7])  for idx in range(len(train_folds)) ]\n# labels\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12, 6))\nplt.subplot(2,1,1)\nplt.bar(labels, len_train_folds, label='number of each train fold')\nplt.legend()\nplt.subplot(2,1,2)\nplt.bar(labels, len_test_folds, label='number of each test fold')\nplt.legend()\nplt.show()","execution_count":9},{"metadata":{"id":"3B24AE89D2C64FA594C0C5074C110BEF","mdEditEnable":false},"cell_type":"markdown","source":"从上图可以看出，类别多包含62种，且每个类别的数目差异大、不均衡。总体特点，多类少样本。"},{"metadata":{"id":"3A44936BAD8A43938A1F79222B514658","mdEditEnable":false},"cell_type":"markdown","source":"# **二、搭建CNN模型分类**"},{"metadata":{"id":"F86F6E7BF7E24A848E233B18E1A54A0C","mdEditEnable":false},"cell_type":"markdown","source":"用深度学习做图片分类考虑CNN模型，主流CNN模型多，在实验之前，难以知道哪个模型效果好。因此，先选择一个简单又经典的模型看一下分类效果，首先选用LeNet实现一下。"},{"metadata":{"id":"3E9CA11E88094A809A4D71A578335274","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n","name":"stderr"},{"output_type":"stream","text":"time: 14.8 s\n","name":"stdout"}],"source":"# import the necessary packages\r\nfrom keras.models import Sequential\r\nfrom keras.layers.convolutional import Conv2D\r\nfrom keras.layers.convolutional import MaxPooling2D\r\nfrom keras.layers.core import Activation\r\nfrom keras.layers.core import Flatten\r\nfrom keras.layers.core import Dense\r\nfrom keras import backend as K\r\n \r\nclass LeNet:\r\n    @staticmethod\r\n    def build(width, height, depth, classes):\r\n        # initialize the model\r\n        model = Sequential()\r\n        inputShape = (height, width, depth)\r\n        # if we are using \"channels last\", update the input shape\r\n        if K.image_data_format() == \"channels_first\":   #for tensorflow\r\n            inputShape = (depth, height, width)\r\n        # first set of CONV => RELU => POOL layers\r\n        model.add(Conv2D(20, (5, 5),padding=\"same\",input_shape=inputShape))\r\n        model.add(Activation(\"relu\"))\r\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n        #second set of CONV => RELU => POOL layers\r\n        model.add(Conv2D(50, (5, 5), padding=\"same\"))\r\n        model.add(Activation(\"relu\"))\r\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n        # first (and only) set of FC => RELU layers\r\n        model.add(Flatten())\r\n        model.add(Dense(500))\r\n        model.add(Activation(\"relu\"))\r\n\r\n        # softmax classifier\r\n        model.add(Dense(classes))\r\n        model.add(Activation(\"softmax\"))\r\n\r\n        # return the constructed network architecture\r\n        return model","execution_count":10},{"metadata":{"id":"43960E76DEFA457E88227ECFB08B1C21","mdEditEnable":false},"cell_type":"markdown","source":"参数解析器和一些参数的初始化"},{"metadata":{"id":"59AF05AD47A34B738C34CA14126B019A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Collecting imutils\n  Downloading https://files.pythonhosted.org/packages/b5/94/46dcae8c061e28be31bcaa55c560cb30ee9403c9a4bb2659768ec1b9eb7d/imutils-0.5.3.tar.gz\nBuilding wheels for collected packages: imutils\n  Running setup.py bdist_wheel for imutils ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/kesci/.cache/pip/wheels/16/84/1f/bf88641293cda2c8be81a5c4b8ca973dd9125a6dc3767417fd\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.3\n\u001b[33mYou are using pip version 9.0.1, however version 19.2.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\ntime: 20.7 s\n","name":"stdout"}],"source":"!pip install imutils","execution_count":11},{"metadata":{"id":"DDD9E113C53042C98C7D53A59D13DCC6","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 3.81 s\n","name":"stdout"}],"source":"'''\r\nhttps://www.cnblogs.com/skyfsm/p/8051705.html\r\n'''\r\n# set the matplotlib backend so figures can be saved in the background\r\nimport matplotlib\r\nmatplotlib.use(\"Agg\")\r\n \r\n# import the necessary packages\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.optimizers import Adam\r\nfrom sklearn.model_selection import train_test_split\r\nfrom keras.preprocessing.image import img_to_array\r\nfrom keras.utils import to_categorical\r\nfrom imutils import paths\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport argparse\r\nimport random\r\nimport cv2\r\nimport os\r\nimport sys\r\nsys.path.append('..')\r\n# from net.lenet import LeNet\r\n\r\n\r\n\r\ndef args_parse():\r\n    # construct the argument parse and parse the arguments\r\n    ap = argparse.ArgumentParser()\r\n    ap.add_argument(\"-dtest\", \"--dataset_test\", required=True,\r\n        help=\"path to input dataset_test\")\r\n    ap.add_argument(\"-dtrain\", \"--dataset_train\", required=True,\r\n        help=\"path to input dataset_train\")\r\n    ap.add_argument(\"-m\", \"--model\", required=True,\r\n        help=\"path to output model\")\r\n    ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\r\n        help=\"path to output accuracy/loss plot\")\r\n    args = vars(ap.parse_args()) \r\n    return args","execution_count":12},{"metadata":{"id":"B451B98C48124DD0A292938D1C7A66A3","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 596 µs\n","name":"stdout"}],"source":"# initialize the number of epochs to train for, initial learning rate,\n# and batch size\nEPOCHS = 35\nINIT_LR = 1e-3\nBS = 32\nCLASS_NUM = 62\nnorm_size = 32","execution_count":13},{"metadata":{"id":"A7A8E5D06C124685BD1DFA60B30A59DE","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fda30ad4f60>"},"execution_count":14},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 2 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/A7A8E5D06C124685BD1DFA60B30A59DE/pxnmuhay9m.png\">"}},{"output_type":"stream","text":"time: 696 ms\n","name":"stdout"}],"source":"from PIL import Image\nimport numpy as np\nimport cv2\nfrom keras.preprocessing.image import img_to_array\n\nimage = cv2.imread(fold_files[0])\n\n# np.array(Image.open(fold_files[0])).shape, image.shape\npilimage = np.array(Image.open(fold_files[0]))\nimage0 = cv2.resize(image, (32, 32))\nimage = img_to_array(image0)\n# image.shape\nplt.subplot(2,1,1)\nplt.imshow(pilimage)\nplt.subplot(2,1,2)\nplt.imshow(image0)\n# image-image0","execution_count":14},{"metadata":{"id":"E9A7F0364907443689DA1D34660FE9A4","mdEditEnable":false},"cell_type":"markdown","source":"# **三、载入数据**\n读入数据，对读入的数据进行处理，并读入相应的标签信息。"},{"metadata":{"id":"77E1C396176041BC8245217C906E1F21","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 1.82 ms\n","name":"stdout"}],"source":"def load_data(path):\n    print('[INFO] loading images...')\n    data = []\n    labels = []\n    # grab the image paths and randomly shuffle them\n    imagePaths = sorted(list(paths.list_images(path)))\n    random.seed(42)\n    random.shuffle(imagePaths)\n    # loop over the input images\n    for imagePath in imagePaths:\n        # load the image.preprocess it and sotre it in the data list\n        image = cv2.imread(imagePath)\n        image = cv2.resize(image, (norm_size, norm_size))\n        image = img_to_array(image)\n        data.append(image)\n        \n        # extract the class label from the image path and update the labels list\n        label = int(imagePath.split(os.path.sep)[-2])\n        labels.append(label)\n    \n    # scale the raw pixel intensities to the image [0,1]\n    data = np.array(data, dtype='float')/255.0\n    labels = np.array(labels)\n    \n    # convert the labels from integers to vectores\n    labels = to_categorical(labels, num_classes=CLASS_NUM)\n    return data,labels","execution_count":15},{"metadata":{"id":"8CC5D6959B4741ED8A5280AAB97BBF4E","mdEditEnable":false},"cell_type":"markdown","source":"训练模型"},{"metadata":{"id":"C50792C7200B4E058021C8F7C9F4FB54","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 1.83 ms\n","name":"stdout"}],"source":"# def train(aug, trainX, trainY, testX, testY, args):\n#     # initialize the model\n#     print('[INFO] compiling model....')\n#     model = LeNet.build(width=norm_size, height=norm_size, depth=3\n#     ,classes=CLASS_NUM)\n#     opt = Adam(lr=INIT_LR, decay=INIT_LR/EPOCHS)\n#     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n#     # train the network\n#     print('[INFO] training network...')\n#     H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n#     validation_data=(trainX, trainY), steps_per_epoch=len*(trainX)//BS,\n#     epochs=EPOCHS, verbose=1)\n#     # save the model to disk\n#     print(['[INFO] serializing network...'])\n#     model.save(args['model'])\n#     # plot the training loss and accuracy\n#     plt.style.use('ggplot')\n#     plt.figure()\n#     N = EPOCHS\n#     plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n#     plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n#     plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n#     plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n#     plt.title(\"Training Loss and Accuracy on traffic-sign classifier\")\n#     plt.xlabel(\"Epoch #\")\n#     plt.ylabel(\"Loss/Accuracy\")\n#     plt.legend(loc=\"lower left\")\n#     plt.savefig(args[\"plot\"])\n    ","execution_count":16},{"metadata":{"id":"40F6AAC4A41A48418E99F850E68DC77E","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 2.93 ms\n","name":"stdout"}],"source":"def train(aug,trainX,trainY,testX,testY,args):\n    # initialize the model\n    print(\"[INFO] compiling model...\")\n    model = LeNet.build(width=norm_size, height=norm_size, depth=3, classes=CLASS_NUM)\n    opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n        metrics=[\"accuracy\"])\n\n    # train the network\n    print(\"[INFO] training network...\")\n    H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n        validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n        epochs=EPOCHS, verbose=1)\n\n    # save the model to disk\n    print(\"[INFO] serializing network...\")\n    model.save(args[\"model\"])\n    \n    # plot the training loss and accuracy\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    N = EPOCHS\n    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n    plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n    plt.title(\"Training Loss and Accuracy on traffic-sign classifier\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss/Accuracy\")\n    plt.legend(loc=\"lower left\")\n    # plt.savefig(args[\"plot\"])","execution_count":17},{"metadata":{"id":"CA1E26E7D2294D12803A2AE36E2FAA1D","mdEditEnable":false},"cell_type":"markdown","source":"# **四、主函数**\n在正式训练之前我们还使用了数据增广技术（ImageDataGenerator）来对我们的小数据集进行数据增强（对数据集图像进行随机旋转、移动、翻转、剪切等），以加强模型的泛化能力。\n\n训练代码已经写好了，接下来开始训练（图片归一化尺寸为32，batch_size为32，epoches为35）。"},{"metadata":{"id":"AB135995100C4186B7D4841928198C1E","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[INFO] loading images...\n[INFO] loading images...\n[INFO] compiling model...\n[INFO] training network...\nEpoch 1/35\n142/142 [==============================] - 34s 241ms/step - loss: 2.6403 - acc: 0.3545 - val_loss: 1.7040 - val_acc: 0.5290\nEpoch 2/35\n142/142 [==============================] - 32s 227ms/step - loss: 1.5016 - acc: 0.5796 - val_loss: 1.0115 - val_acc: 0.6937\nEpoch 3/35\n142/142 [==============================] - 32s 227ms/step - loss: 1.0556 - acc: 0.6932 - val_loss: 0.7800 - val_acc: 0.7544\nEpoch 4/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.8719 - acc: 0.7325 - val_loss: 0.5763 - val_acc: 0.8238\nEpoch 5/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.7279 - acc: 0.7736 - val_loss: 0.4912 - val_acc: 0.8508\nEpoch 6/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.6170 - acc: 0.8086 - val_loss: 0.6251 - val_acc: 0.7972\nEpoch 7/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.5595 - acc: 0.8327 - val_loss: 0.4640 - val_acc: 0.8591\nEpoch 8/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.4946 - acc: 0.8453 - val_loss: 0.3826 - val_acc: 0.8798\nEpoch 9/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.4386 - acc: 0.8574 - val_loss: 0.4005 - val_acc: 0.8770\nEpoch 10/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.4410 - acc: 0.8625 - val_loss: 0.3499 - val_acc: 0.9056\nEpoch 11/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.3609 - acc: 0.8920 - val_loss: 0.3330 - val_acc: 0.9008\nEpoch 12/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.3264 - acc: 0.8937 - val_loss: 0.2550 - val_acc: 0.9294\nEpoch 13/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.3251 - acc: 0.8997 - val_loss: 0.2660 - val_acc: 0.9179\nEpoch 14/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.2973 - acc: 0.9084 - val_loss: 0.3615 - val_acc: 0.8976\nEpoch 15/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.2635 - acc: 0.9155 - val_loss: 0.3430 - val_acc: 0.9103\nEpoch 16/35\n142/142 [==============================] - 32s 225ms/step - loss: 0.2484 - acc: 0.9238 - val_loss: 0.2465 - val_acc: 0.9361\nEpoch 17/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.2432 - acc: 0.9225 - val_loss: 0.2533 - val_acc: 0.9349\nEpoch 18/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.2348 - acc: 0.9302 - val_loss: 0.2782 - val_acc: 0.9214\nEpoch 19/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.2174 - acc: 0.9312 - val_loss: 0.2563 - val_acc: 0.9306\nEpoch 20/35\n142/142 [==============================] - 32s 226ms/step - loss: 0.1929 - acc: 0.9416 - val_loss: 0.2991 - val_acc: 0.9171\nEpoch 21/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.1848 - acc: 0.9458 - val_loss: 0.2403 - val_acc: 0.9329\nEpoch 22/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.1862 - acc: 0.9412 - val_loss: 0.4600 - val_acc: 0.8667\nEpoch 23/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.1756 - acc: 0.9449 - val_loss: 0.2925 - val_acc: 0.9254\nEpoch 24/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.1733 - acc: 0.9484 - val_loss: 0.2628 - val_acc: 0.9317\nEpoch 25/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.1600 - acc: 0.9465 - val_loss: 0.3680 - val_acc: 0.8972\nEpoch 26/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.1461 - acc: 0.9546 - val_loss: 0.2388 - val_acc: 0.9349\nEpoch 27/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.1534 - acc: 0.9484 - val_loss: 0.2964 - val_acc: 0.9246\nEpoch 28/35\n142/142 [==============================] - 33s 233ms/step - loss: 0.1543 - acc: 0.9535 - val_loss: 0.3168 - val_acc: 0.9131\nEpoch 29/35\n142/142 [==============================] - 34s 239ms/step - loss: 0.1473 - acc: 0.9533 - val_loss: 0.3164 - val_acc: 0.9119\nEpoch 30/35\n142/142 [==============================] - 34s 238ms/step - loss: 0.1437 - acc: 0.9575 - val_loss: 0.2563 - val_acc: 0.9389\nEpoch 31/35\n142/142 [==============================] - 34s 237ms/step - loss: 0.1241 - acc: 0.9637 - val_loss: 0.2330 - val_acc: 0.9365\nEpoch 32/35\n142/142 [==============================] - 33s 232ms/step - loss: 0.1227 - acc: 0.9638 - val_loss: 0.2379 - val_acc: 0.9401\nEpoch 33/35\n142/142 [==============================] - 32s 228ms/step - loss: 0.1351 - acc: 0.9583 - val_loss: 0.2309 - val_acc: 0.9472\nEpoch 34/35\n142/142 [==============================] - 32s 227ms/step - loss: 0.1299 - acc: 0.9626 - val_loss: 0.2567 - val_acc: 0.9393\nEpoch 35/35\n142/142 [==============================] - 33s 233ms/step - loss: 0.0963 - acc: 0.9731 - val_loss: 0.2937 - val_acc: 0.9226\n[INFO] serializing network...\n","name":"stdout"},{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/AB135995100C4186B7D4841928198C1E/pxnnqalvj9.png\">"}},{"output_type":"stream","text":"time: 19min 5s\n","name":"stdout"}],"source":"#python train.py --dataset_train ../../traffic-sign/train --dataset_test ../../traffic-sign/test --model traffic_sign.model\nif __name__=='__main__':\n    # args = args_parse()\n    # train_file_path = args[\"dataset_train\"]\n    train_file_path = '/home/kesci/input/traffic_sign8499/traffic-sign/train'\n    # test_file_path = args[\"dataset_test\"]\n    test_file_path = '/home/kesci/input/traffic_sign8499/traffic-sign/test'\n    args = {}\n    args['model'] = 'traffic_sign_v1.model'\n    trainX,trainY = load_data(train_file_path)\n    testX,testY = load_data(test_file_path)\n    # construct the image generator for data augmentation\n    aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n        height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n        horizontal_flip=True, fill_mode=\"nearest\")\n    train(aug,trainX,trainY,testX,testY,args)","execution_count":18},{"metadata":{"id":"E39104151F5B4CFEBEC1818D16512A78","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"# **五、预测单张图片\n现在我们已经得到了我们训练好的模型traffic_sign.model，然后我们编写一个专门用于预测的脚本predict.py。\n预测脚本中的代码编写思路是：参数解析器-》载入训练好的模型-》读入图片信息-》预测-》展示预测效果。值得注意的是，参数-s是用于可视化结果的，加上他的话我们就可以看出我们输入的图片以及模型预测的分类结果，很直观。如果只需要得到分类结果，不加-s就可以了。"},{"metadata":{"id":"87DAB9ECD83441AE930E7C45517633ED","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 685 µs\n","name":"stdout"}],"source":"# # import the necessary packages\n# from keras.preprocessing.image import img_to_array\n# from keras.models import load_model\n# import numpy as np\n# import argparse\n# import imutils\n# import cv2\n\n# norm_size = 32\n\n# def args_parse():\n# # construct the argument parse and parse the arguments\n#     ap = argparse.ArgumentParser()\n#     ap.add_argument(\"-m\", \"--model\", required=True,\n#         help=\"path to trained model model\")\n#     ap.add_argument(\"-i\", \"--image\", required=True,\n#         help=\"path to input image\")\n#     ap.add_argument(\"-s\", \"--show\", action=\"store_true\",\n#         help=\"show predict image\",default=False)\n#     args = vars(ap.parse_args())    \n#     return args\n\n# def predict(args):\n#     print('[INFO] loading network...')\n#     model = load_model(args['model'])\n    \n#     # load the iamge\n#     image = cv2.imread(args['image'])\n#     orig = image.copy()\n    \n#     # pre-processing the image for classifcation\n#     image = cv2.resize(image, (norm_size, norm_size))\n#     image = image.astype('float')/255.0\n#     image = img_to_array(image)\n#     image = np.expand_dims(image, axis=0)\n    \n#     # classify the input image\n#     result = model.predict(image)[0]\n    \n#     proba = np.max(result)\n#     label = str(np.where(result==proba)[0])\n#     label = \"{}: {:.2f}%\".format(label, proba * 100)\n#     print(label)\n    \n#     if args['show']:   \n#         # draw the label on the image\n#         output = imutils.resize(orig, width=400)\n#         cv2.putText(output, label, (10, 25),cv2.FONT_HERSHEY_SIMPLEX,\n#             0.7, (0, 255, 0), 2)       \n#         # show the output image\n#         cv2.imshow(\"Output\", output)\n#         cv2.waitKey(0)\n\n\n# #python predict.py --model traffic_sign.model -i ../2.png -s\n# if __name__ == '__main__':\n#     # args = args_parse()\n#     args = {}\n#     args['model']='traffic_sign.model'\n#     args['show']=True\n#     args['image'] = '/home/kesci/input/traffic_sign8499/traffic-sign/test/00000/00017_00000.png'\n#     predict(args)\n    ","execution_count":19},{"metadata":{"id":"9E6F044CAF414387A3860787C26E12D2","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[INFO] loading network...\n[0]: 99.95%\n","name":"stdout"},{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9E6F044CAF414387A3860787C26E12D2/pxnnqc2b99.png\">"}},{"output_type":"stream","text":"time: 1.45 s\n","name":"stdout"}],"source":"# import the necessary packages\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import load_model\nimport numpy as np\nimport argparse\nimport imutils\nimport cv2\n\nnorm_size = 32\n\ndef args_parse():\n# construct the argument parse and parse the arguments\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"-m\", \"--model\", required=True,\n        help=\"path to trained model model\")\n    ap.add_argument(\"-i\", \"--image\", required=True,\n        help=\"path to input image\")\n    ap.add_argument(\"-s\", \"--show\", action=\"store_true\",\n        help=\"show predict image\",default=False)\n    args = vars(ap.parse_args())    \n    return args\n\n    \ndef predict(args):\n    # load the trained convolutional neural network\n    print(\"[INFO] loading network...\")\n    model = load_model(args[\"model\"])\n    \n    #load the image\n    image = cv2.imread(args[\"image\"])\n    orig = image.copy()\n     \n    # pre-process the image for classification\n    image = cv2.resize(image, (norm_size, norm_size))\n    image = image.astype(\"float\") / 255.0\n    image = img_to_array(image)\n    image = np.expand_dims(image, axis=0)\n     \n    # classify the input image\n    result = model.predict(image)[0]\n    #print (result.shape)\n    proba = np.max(result)\n    label = str(np.where(result==proba)[0])\n    label = \"{}: {:.2f}%\".format(label, proba * 100)\n    print(label)\n    \n    if args['show']:   \n        # draw the label on the image\n        output = imutils.resize(orig, width=400)\n        cv2.putText(output, label, (10, 25),cv2.FONT_HERSHEY_SIMPLEX,\n            0.7, (0, 255, 0), 2)       \n        # show the output image\n        # cv2.imshow(\"Output\", output)\n        # cv2.waitKey(0)\n        plt.imshow(output)\n\n\n#python predict.py --model traffic_sign.model -i ../2.png -s\nif __name__ == '__main__':\n    # args = args_parse()\n    args = {}\n    args['model']='traffic_sign.model'\n    args['show']=True\n    args['image'] = '/home/kesci/input/traffic_sign8499/traffic-sign/test/00000/00017_00000.png'\n    predict(args)","execution_count":20},{"metadata":{"id":"E5567FA0E8124D649AC07B8B2207A362","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":7},{"metadata":{"id":"DADEA877747548A7955CE4352088FBD8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":8},{"metadata":{"id":"CF319AF3FF1443A68DC5B377D9BAA578","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":9},{"metadata":{"id":"FCD1601DB0524B1989D03AA50C0DA498","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":10},{"metadata":{"id":"306AD1440C9640BAB37EB8C8F1F8640E","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":11},{"metadata":{"id":"688C070F608D4D979360F5BE6221855D","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}